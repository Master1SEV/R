Sys.which("make")
# Данные: Зависимость катодных пятен от скорости наплавки 
d <- read.csv("...\\dataset.csv", sep = ";", header = TRUE, dec = ",")
# Присвоение данных переменным: y - Скорость сварки, м/час, х - ширина катодных пятен, м
x <- as.double(d$Bo)
y <- as.double(d$Vsv)
# Стандартное отклонение x и y
sdx <- sd(x)
sdy <- sd(y)
# стандартной ошибки средней x и y
SEmpgX = sd(x)/sqrt(length(x))
SEmpgY = sd(y)/sqrt(length(y))
# порядковые номера элементов, обладающих минимальным и максимальным значениями 
which.min(x)
which.max(x)
which.min(y)
which.max(y)
e1 <- data.frame(x, y)
n <- 5 # Степень полинома
RSS <- function(x,y,n)
{
  x1 <- x+1-min(x)
  lx <- length(x)
  m1 <- matrix(1,lx,n+1)
  for(i in 1:lx)
  {
    m1[i,] <- x1[i]^(0:n)
  }
  a <- solve(t(m1) %*% m1) %*% t(m1) %*% as.matrix(y)
  a
  # Генерация Последовательности
  x2 <- seq(x1[1] - 1, x1[length(x1)] +1, 0.1)
  x2
  # Генерация Последовательности
  y2 <- c(NULL)
  for (i in 1:length(x2)) 
  {
    y2[i] <- sum(x2[i]^(0:n) * a)
    y2[i]
  }
  y2
  # Аппроксимирующая кривая
  # будет проходить ближе к точкам с большим весом
  plot(x2 + min(x) - 1, y2, type = "l", col = 3, xlab = "x", ylab = "y", main = paste("Полином", n, "степени"))
  # Аппроксимирующая прямая
  regress=lm(formula = y ~ x)
  abline(regress,col="orange",lwd="3",add=TRUE)
  # Исходные данные
  points(x, y)
  # Подогнанные Y к аппроксимирующей функции
  y3 <- c(NULL)
  for (i in 1:length(x1))
  {
    y3[i] <- sum(x1[i]^(0:n) * a)
    y3
  }
  y4 <- sqrt(sum((y - y3)**2)/length(y))
  y4
  # Подогнанные Y к аппроксимирующей функции(Синие точки)
  i <- x1 + min(x) - 1
  y3
  points(i, y3, col = 4, pch = 20) 
  # Исходные данные (Красные точки)
  points(x, y, pch = 20, col = 2)
  
  i1 <- i-x
  i2 <- y3-y
  e2 <- data.frame(i1, i2)
  # Доверительный интервал (Красный цвет графика)
  # Исходная функция с вероятностью 95% проходит через этот интервал
  i2 <- y2 + y4
  i2
  points(x2 + min(x) - 1, y2 + y4, type = "l", col = "red")
  points(x2 + min(x) - 1, y2 - y4, type = "l", col = "red")
  # Интервал предсказания (Желтый цвет графика)
  # Новая точка попадает в этот интервал с вероятностью 95%
  points(x2 + min(x) - 1, y2 + 2*y4, type = "l", col = "yellow")
  points(x2 + min(x) - 1, y2 - 2*y4, type = "l", col = "yellow")
}
dd <- par(mfrow = c(1,2))
RSS(x,y,n)
# Точки № 5 (28:40) и №8 (30:16) имеют высокую разность между исходными данными и подогнанными данными -4.9463363 и 8.0491515 соответственно (таблица e2 i2 = (y3-y))
RSS(x[-n],y[-n],n)
model <- lm(x ~ y, data = d)
coef(model)
# Доверительные интервалы для коэффициентов
confint <- confint(model, level = 0.95)
confint
t_test <- t.test(x, y, paired = TRUE)
t_test
#mahalanobis(x, center, cov, inverted = FALSE, ...)
# Формально критерий МНК можно записать так = sum(y-y_hat)^2 -> min
# МНК не требует никакой априорной информации т.е. достаточно чтобы быловыполнено условие  J = (sum(y-y_hat)^2)^1/2 -> min
y_hat
yy <- y-y_hat
yy <- sum(yy)
yy <- yy^2
yy
# 
nn <- 15
# nn <- 12
# Суммы Ср. значений
xq <- y^2
yq <- x^2
xy <- x*y
x_hat <- (sum(x)/15)
y_hat <- (sum(y)/15)
xy_hat <- (sum(xy)/15)
e <- data.frame(y, x, xq, yq, xy)
# Ср. значения: x, y, x^2, y^2, x*y
Srzx <- mean(e$y)
Srzx
Srzy <- mean(e$x)
Srzy
Srzxq <- mean(e$xq)
Srzxq
Srzyq <- mean(e$yq)
Srzyq
Srzxy <- mean(e$xy)
Srzxy
Syyx2 <- y-y
Srzframe <- data.frame(Srzx, Srzy, Srzxq, Srzyq, Srzxy)
# Параметры уравнения регрессии
b <- (Srzxy-(Srzx*Srzy))/(Srzxq-(Srzx^2))
b
a <- Srzy-b*Srzx
a
# Уравнение регрессии (эмпирическое уравнение регрессии)
# Yx = -0.25*x+38.59
Yx <- b*y+a
Yx
# Коэффициент линейной парной корреляции
Rxy <- b*Sx/Sy
# Rxy = -0.932823 => связь между признаком Y и фактором X высокая и обратная по шкале Чеддока.
# Дисперсия
Sumx <- (sum(xq))
Sumx
Sumy <- (sum(yq))
Sumy
S2x <- ((Sumx/nn)-(Srzx^(2)))
S2x
S2y <- ((Sumy/nn)-(Srzy^(2)))
S2y
S2xy <- sum(xy)/nn
Sumframe <- data.frame(Sumx,Sumy,S2x,S2y,S2xy)
# Среднеквадратическое отклонение/стандартная ошибка оценки (стандартная ошибка регрессии)
Sx <- sqrt(S2x)
Sx
Sy <- sqrt(S2y)
Sy
# Необъясненная дисперсия или дисперсия ошибки регрессии
Y2 <- (x-Yx)^2
Y2 <- sum(Y2)
Y2
Ysr <- (x-Srzy)^2
Ysr <- sum(Ysr)
Ysr
Sq <- Y2/13
Sq
# Cтандартная ошибка оценки
Ssqrt <- sqrt(Sq)
Ssqrt
# стандартное отклонение случайной величины a.
Sa <- (Ssqrt*(sqrt(Sumx))/(nn*Sx))
Sa
# стандартное отклонение случайной величины b.
Sb <- ((Ssqrt)/(sqrt(nn)*Sx))
Sb 

# Проверка гипотезы H0 о равенстве отдельных коэффициентов регрессии нулю на уровне значимости 0.05
# Для проверки этой гипотезы используется t-критерий Стьюдента
# t-критерий стьюдента 
# (Степеней свободы k = n-m-1=13 (10), уровень значимости 0,05 )
Tтабл <- 2.16
Tb <- b/Sb
# |Tb | = 9.3 > 2.16 статистическая значимость коэффициента регрессии b подтверждается
# отвергаем гипотезу о равенстве нулю этого коэффициента
# коэффициент автокорреляции статистически - значим
Ta <- a/Sa
Ta
# Поскольку 38 > 2.16, то статистическая значимость коэффициента регрессии a подтверждается
#  отвергаем гипотезу о равенстве нулю этого коэффициента
#  коэффициент автокорреляции статистически - значим
T <- ((x_hat - 1)/(Sx/(sqrt(nn))))
# Поскольку 4.61 > 2.16, то статистическая значимость коэффициента регрессии a подтверждается
#  отвергаем гипотезу о равенстве нулю этого коэффициента
#  коэффициент автокорреляции статистически - значим

#  Критерий Фишера 
k1 <- 1 
k2 <- 12 
Ftab <- 4.67
#  Коэффициент детерминации 
R2 <- 1-(Y2/Ysr)
R2
# фактическое значение F-критерия
Fr <- (R2*(nn-1-1))/(1-R2)
Fr
# Поскольку фактическое значение Fr = 87.122 > Ftab = 4.67 
# то коэффициент детерминации статистически значим 
# найденная оценка уравнения регрессии статистически надежна
# В исследуемой ситуации 87.015% общей вариабельности Y объясняется изменением X. 

tab1 <- data.frame(Fr,T ,Tb, Ta)

# Среднеквадратическое отклонение
Sx <- sqrt(S2x)
Sy <- sqrt(S2y)

# Ошибка прогноза
E <- (y-y3)
rss <- sum(E*E)
# Сумма квадратов ошибок прогноза (Меньшее значение - лучше)
E2 <- sum((y-y3)^2)
#  E2 = 177,7049 Сумма квадратов ошибок прогноза.
#  E2 = 49.17103 Сумма квадратов ошибок прогноза при исключенных данных, выходящих за пределы доверительного интервала. 
########################################
#  Данные с исключенными 5 точки (16;30), выходившей за пределы доверительного интервала 
d <- read.csv("C:\\Users\\Evgenii\\Documents\\SMIA\\3\\3del.csv", sep = ";", header = TRUE, dec = ",")
# Присвоение данных переменным: y - Скорость сварки, м/час, х - ширина катодных пятен, м
x <- as.double(d$Bo)
y <- as.double(d$Vsv)
#yrand <- runif(15, min = 0.5, max = 80)
#yrand
n <- 5 # Степень полинома
RSS <- function(x,y,n)
{
  x1 <- x+1-min(x)
  lx <- length(x)
  m1 <- matrix(1,lx,n+1)
  for(i in 1:lx)
  {
    m1[i,] <- x1[i]^(0:n)
  }
  a <- solve(t(m1) %*% m1) %*% t(m1) %*% as.matrix(y)
  a
  x2 <- seq(x1[1] - 1, x1[length(x1)] +1, 0.1)
  y2 <- c(NULL)
  for (i in 1:length(x2)) 
  {
    y2[i] <- sum(x2[i]^(0:n) * a)
    y2[i]
  }
  # Аппроксимирующая функция
  plot(x2 + min(x) - 1, y2, type = "l", col = 3, xlab = "x", ylab = "y", main = paste("Полином", n, "степени"))
  #plot(x, yrand, type = "l", col = 3, xlab = "x", ylab = "y", main = paste("Полином", n, "степени"))
  # Исходные данные
  points(x, y)
  # Подогнанные Y к аппроксимирующей функции
  y3 <- c(NULL)
  for (i in 1:length(x1))
  {
    y3[i] <- sum(x1[i]^(0:n) * a)
    y3
  }
  y4 <- sqrt(sum((y - y3)**2)/length(y))
  y4
  # Подогнанные Y к аппроксимирующей функции(Синие точки)
  points(x1 + min(x) - 1, y3, col = 4, pch = 20) 
  # Исходные данные (Красные точки)
  points(x, y, pch = 20, col = 2)
  i1 <- i-x
  i2 <- y3-y
  e2 <- data.frame(i1, i2)
  # Доверительный интервал (Красный цвет графика)
  # Исходная функция с вероятностью 95% проходит через этот интервал
  points(x2 + min(x) - 1, y2 + y4, type = "l", col = "red")
  points(x2 + min(x) - 1, y2 - y4, type = "l", col = "red")
  # Интервал предсказания (Желтый цвет графика)
  # Новая точка попадает в этот интервал с вероятностью 95%
  points(x2 + min(x) - 1, y2 + 2*y4, type = "l", col = "yellow")
  points(x2 + min(x) - 1, y2 - 2*y4, type = "l", col = "yellow")
}
dd <- par(mfrow = c(1,2))
RSS(x,y,3)
RSS(x[-3],y[-3],3)

y_hat
yy <- y-y_hat
yy <- sum(yy)
yy <- yy^2
yy
# 
nn <- 12
# nn <- 12
# Суммы Ср. значений
xq <- y^2
yq <- x^2
xy <- x*y
x_hat <- (sum(x)/nn)
y_hat <- (sum(y)/nn)
xy_hat <- (sum(xy)/nn)
e <- data.frame(y, x, xq, yq, xy)
# Ср. значения: x, y, x^2, y^2, x*y
Srzx <- mean(e$y)
Srzx
Srzy <- mean(e$x)
Srzy
Srzxq <- mean(e$xq)
Srzxq
Srzyq <- mean(e$yq)
Srzyq
Srzxy <- mean(e$xy)
Srzxy
Srzframe <- data.frame(Srzx, Srzy, Srzxq, Srzyq, Srzxy)
# Параметры уравнения регрессии
b <- (Srzxy-(Srzx*Srzy))/(Srzxq-(Srzx^2))
b
a <- Srzy-b*Srzx
a
# Уравнение регрессии (эмпирическое уравнение регрессии)
# Yx = -0.25*x+38.59
Yx <- b*y+a
Yx
# Коэффициент линейной парной корреляции
Rxy <- b*Sx/Sy
Rxy
# Rxy = -0.95 => связь между признаком Y и фактором X высокая и обратная по шкале Чеддока.
# Дисперсия
Sumx <- (sum(xq))
Sumx
Sumy <- (sum(yq))
Sumy
S2x <- ((Sumx/nn)-(Srzx^(2)))
S2x
S2y <- ((Sumy/nn)-(Srzy^(2)))
S2y
S2xy <- sum(xy)/nn
Sumframe <- data.frame(Sumx,Sumy,S2x,S2y,S2xy)
# Среднеквадратическое отклонение/стандартная ошибка оценки (стандартная ошибка регрессии)
Sx <- sqrt(S2x)
Sx
Sy <- sqrt(S2y)
Sy
# Необъясненная дисперсия или дисперсия ошибки регрессии
Y2 <- (x-Yx)^2
Y2 <- sum(Y2)
Y2
Ysr <- (x-Srzy)^2
Ysr <- sum(Ysr)
Ysr
Sq <- Y2/13
Sq
# Cтандартная ошибка оценки
Ssqrt <- sqrt(Sq)
Ssqrt
# стандартное отклонение случайной величины a.
Sa <- (Ssqrt*(sqrt(Sumx))/(nn*Sx))
Sa
# стандартное отклонение случайной величины b.
Sb <- ((Ssqrt)/(sqrt(nn)*Sx))
Sb 

# Проверка гипотезы H0 о равенстве отдельных коэффициентов регрессии нулю на уровне значимости 0.05
#  Для проверки этой гипотезы используется t-критерий Стьюдента
# t-критерий стьюдента 
# (Степеней свободы k = n-m-1=13 (10), уровень значимости 0,05 )
Tтабл <- 2.16
Tb2 <- b/Sb
Tb2
#  |Tb | =  8,64 > 2.16 статистическая значимость коэффициента регрессии b  подтверждается
#  непринимаем гипотезу о равенстве нулю этого коэффициента
#  коэффициент автокорреляции статистически - значим
Ta2 <- a/Sa
Ta2
# Поскольку 38,5 > 2.16, то статистическая значимость коэффициента регрессии a подтверждается
#  отвергаем гипотезу о равенстве нулю этого коэффициента
#  коэффициент автокорреляции статистически - значим
T2 <- ((x_hat - 1)/(Sx/(sqrt(nn))))
T2
# Поскольку 4,28 > 2.16, то статистическая значимость коэффициента регрессии a подтверждается
#  отвергаем гипотезу о равенстве нулю этого коэффициента
#  коэффициент автокорреляции статистически - значим

#  Критерий Фишера 
k1 <- 1 
k2 <- 12 
Ftab <- 4.67
#  Коэффициент детерминации 
R22 <- 1-(Y2/Ysr)
R22
# фактическое значение F-критерия
Fr2 <- (R2*(nn-1-1))/(1-R2)
Fr2
# Поскольку фактическое значение Fr = 67,01 > Ftab = 4.67 
# то коэффициент детерминации статистически значим 
# найденная оценка уравнения регрессии статистически надежна
# В исследуемой ситуации 85,17% общей вариабельности Y объясняется изменением X. 
tab2 <- data.frame(Fr2,T2 ,Tb2, Ta2)

